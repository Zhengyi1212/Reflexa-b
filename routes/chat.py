# api/chat.py
from fastapi import APIRouter, HTTPException, status, Depends
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

from services.services import get_vector_store
from utility.schemas import ChatRequest
from utility.config import settings
from utility.prompt import (
    USER_PROMPT,
    GENERAL_SYSTEM_PROMPT,
    EXPLAINABLE_SYSTEM_PROMPT,
    EXPLORATIVE_SYSTEM_PROMPT,
    TRANSFORMATIVE_SYSTEM_PROMPT
)

# â€¼ï¸ ä¿®æ”¹ç‚¹: æ›´æ–°å¯¼å…¥çš„å‡½æ•°
from utility.deep_chat import (
    generate_deep_reflection_response,
    generate_transition_response,
    DEEP_REFLECTION_KEYWORDS,
    generate_vague_deep_reflection_response # å¯¼å…¥é‡æ„åçš„å‡½æ•°
)

router = APIRouter()

# --- LLM è®¾ç½® (ä¿æŒä¸å˜) ---
llm = AzureChatOpenAI(
    openai_api_version=settings.AZURE_OPENAI_API_VERSION,
    azure_deployment=settings.AZURE_OPENAI_MODEL_NAME,
    azure_endpoint=settings.AZURE_OPENAI_ENDPOINT,
    api_key=settings.AZURE_OPENAI_API_KEY,
    temperature=0.7
)

# --- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---
def format_memories_for_prompt(memories: list) -> str:
    if not memories:
        return "No relevant historical versions found."
    formatted = []
    for i, mem in enumerate(memories):
        formatted.append(
            f"Memory {i+1} (Version ID: {mem.get('version_id', 'N/A')}):\n"
            f"- ç‰ˆæœ¬æ€»ç»“: {mem.get('ai_summary', 'N/A')}\n"
        )
    return "\n".join(formatted)

def format_history_for_prompt(history: list) -> str:
    if not history:
        return "No recent conversation history."
    return "\n".join([f"{item['role'].capitalize()}: {item['content']}" for item in history])


@router.post("/chat")
async def chat(
    request: ChatRequest,
    vector_store: Chroma = Depends(get_vector_store)
):
    """
    å¤„ç†èŠå¤©è¯·æ±‚ã€‚
    æ ¹æ®äº¤äº’æ¨¡å¼å’Œæ¬¡æ•°ï¼Œè·¯ç”±åˆ°æ™®é€šèŠå¤©ã€è¿‡æ¸¡å±‚æˆ–æ·±åº¦åæ€ã€‚
    """
    try:
        # --- æ•°æ®å‡†å¤‡ (ä¿æŒä¸å˜) ---
        retrieval_query = f"{request.code_description}\n{request.user_question}"
        where_clause = {
            "$and": [
                {"session_id": {"$eq": request.session_id}},
                {"version_id": {"$ne": request.version_id}}
            ]
        }
        results = vector_store.similarity_search(query=retrieval_query, k=3, filter=where_clause)
        retrieved_metadatas = [doc.metadata for doc in results]
        formatted_memories = format_memories_for_prompt(retrieved_metadatas)
        formatted_history = format_history_for_prompt(request.short_term_history)

        # --- æ ¸å¿ƒè·¯ç”±é€»è¾‘ ---
        if request.type in ['explainable', 'explorative', 'transformative']:
            
            # --- ç¬¬äºŒæ¬¡äº¤äº’: è¿‡æ¸¡å±‚ (ä¿æŒä¸å˜) ---
            if request.interaction_count == 2:
                print(f"ğŸŒ€ [ä¼šè¯: {request.session_id}] è¿›å…¥è¿‡æ¸¡åæ€å±‚ (ç¬¬ {request.interaction_count} æ¬¡)ã€‚")
                transition_response = await generate_transition_response(
                    user_question=request.user_question,
                    current_code=request.code,
                    memory=formatted_memories,
                    history=formatted_history,
                    llm=llm
                )
                transition_sentences = {
                    "explainable": "ğŸ’¡å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä»**åŠ¨æœºè¯´æ˜**,**é˜æ˜ç›®æ ‡**æˆ–**ç»†èŠ‚å†³ç­–è¯´æ˜**é€‰æ‹©ä¸€ä¸ªæ–¹å‘ç»§ç»­è¿›è¡Œæ€è€ƒ",
                    "explorative": "ğŸ’¡å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä»**æ¦‚å¿µè”ç³»æ¢ç´¢**,**æ¨¡å—ä½“éªŒå…³ç³»**æˆ–**æƒ…æ„Ÿè§†è§‰ä¸€è‡´æ€§**é€‰æ‹©ä¸€ä¸ªæ–¹å‘ç»§ç»­è¿›è¡Œæ€è€ƒ",
                    "transformative": "ğŸ’¡å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä»**åˆ›æ„æ–¹æ³•æ”¹å˜**,**åŠŸèƒ½æ–¹æ³•é‡æ€**æˆ–**è§†è§‰é£æ ¼è°ƒæ•´**é€‰æ‹©ä¸€ä¸ªæ–¹å‘ç»§ç»­è¿›è¡Œæ€è€ƒ"
                }
                transition_response['advice'] = transition_sentences.get(request.type, "")
                print(f"âœ… [ä¼šè¯: {request.session_id}] è¿‡æ¸¡å±‚å“åº”å·²ç”Ÿæˆã€‚")
                print(transition_response)
                return transition_response

            # --- â€¼ï¸ ç¬¬ä¸‰æ¬¡åŠä»¥ä¸Šäº¤äº’: æ·±åº¦åæ€ (æ ¸å¿ƒä¿®æ”¹ç‚¹) ---
            elif request.interaction_count >= 3:
                print(f"ğŸš€ [ä¼šè¯: {request.session_id}] è¿›å…¥æ·±åº¦åæ€æ¨¡å¼ (ç¬¬ {request.interaction_count} æ¬¡)ã€‚")
                
                # --- æƒ…å†µ1: æ˜ç¡®æ„å›¾ - é€šè¿‡å…³é”®è¯ç›´æ¥åŒ¹é… ---
                matched_category = None
                keywords_for_mode = DEEP_REFLECTION_KEYWORDS.get(request.type, {})
                for keyword, category in keywords_for_mode.items():
                    if keyword in request.user_question:
                        matched_category = category
                        print(f"ğŸ¯ [ä¼šè¯: {request.session_id}] åŒ¹é…åˆ°å…³é”®è¯ '{keyword}'ï¼Œæ„å›¾æ˜ç¡®ä¸º '{category}'ã€‚")
                        break
                
                if matched_category:
                    reflection_string = await generate_deep_reflection_response(
                        mode=request.type,
                        category=matched_category,
                        history= formatted_history,
                        memory = formatted_memories,
                        llm=llm,
                        
                    )
                    print(f"ğŸ’¬ [ä¼šè¯: {request.session_id}] å·²ç”Ÿæˆæ¨¡æ¿åŒ–åæ€é—®é¢˜ã€‚")
                    return {"reflection": reflection_string}

                # --- æƒ…å†µ2: æ¨¡ç³Šæ„å›¾ - è°ƒç”¨æ–°çš„ç»“æ„åŒ–å“åº”ç”Ÿæˆå™¨ ---
                else:
                    print(f"ğŸŒ€ [ä¼šè¯: {request.session_id}] æœªåŒ¹é…åˆ°å…³é”®è¯ï¼Œå¯åŠ¨æ¨¡ç³Šæ„å›¾ç»“æ„åŒ–å“åº”ã€‚")
                    
                    # ç›´æ¥è°ƒç”¨é‡æ„åçš„å‡½æ•°ï¼Œå®ƒå°†å¤„ç†æ‰€æœ‰é€»è¾‘
                    structured_response = await generate_vague_deep_reflection_response(
                        user_question=request.user_question,
                        current_code=request.code,
                        mode=request.type,
                        llm=llm,
                        history = formatted_history,
                        memory = formatted_memories
                    )
                    
                    print(f"âœ… [ä¼šè¯: {request.session_id}] å·²ç”Ÿæˆå››æ®µå¼æ¨¡ç³Šåæ€å“åº”ã€‚")
                    return structured_response

        # --- æ™®é€šèŠå¤©æµç¨‹ (ä¿æŒä¸å˜) ---
        print(f"ğŸ’¬ [ä¼šè¯: {request.session_id}] æ™®é€šèŠå¤©æ¨¡å¼ (ç¬¬ {request.interaction_count} æ¬¡)ã€‚")
        system_prompts = {
            'explainable': EXPLAINABLE_SYSTEM_PROMPT,
            'explorative': EXPLORATIVE_SYSTEM_PROMPT,
            'transformative': TRANSFORMATIVE_SYSTEM_PROMPT,
            'general': GENERAL_SYSTEM_PROMPT
        }
        system_prompt = system_prompts.get(request.type, GENERAL_SYSTEM_PROMPT)
        chat_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt),
            HumanMessagePromptTemplate.from_template(USER_PROMPT)
        ])
        chain = chat_prompt | llm | JsonOutputParser()
        chain_input = {
            "retrieved_memories": formatted_memories,
            "short_term_history": formatted_history,
            "code_description": request.code_description,
            "current_code": request.code,
            "user_question": request.user_question,
        }
        response = await chain.ainvoke(chain_input)
        print(f"âœ… [ä¼šè¯: {request.session_id}] æ™®é€šèŠå¤©å“åº”å·²ç”Ÿæˆã€‚")
        print(response)
        return response

    except Exception as e:
        print(f"âŒ åœ¨ /chat ç«¯ç‚¹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=f"An internal error occurred while processing the chat: {str(e)}"
        )
